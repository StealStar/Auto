{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN8Yl40jqvL2/vFYqwZZ2Cr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/StealStar/Auto/blob/Colab-code/%EC%9E%90%EB%8F%99%EC%82%AC%EB%83%A5_%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "9XW2Z_GGdjqT",
        "outputId": "8acecff4-12ec-405f-c2d4-63f398d081ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-cloud-texttospeech in /usr/local/lib/python3.11/dist-packages (2.25.1)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-texttospeech) (2.24.2)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-cloud-texttospeech) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-cloud-texttospeech) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-cloud-texttospeech) (4.25.6)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-texttospeech) (1.69.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-texttospeech) (2.32.3)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-texttospeech) (1.71.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-texttospeech) (1.62.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-texttospeech) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-texttospeech) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-texttospeech) (4.9)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-texttospeech) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-texttospeech) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-texttospeech) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-texttospeech) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-texttospeech) (2025.1.31)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.61.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.10.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install google-cloud-texttospeech  # TTS ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
        "!pip install openai  # ë˜ëŠ” ë‹¤ë¥¸ AI API ë¼ì´ë¸ŒëŸ¬ë¦¬"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ê¸°ì¡´ì— ì‘ì„±í•˜ì‹  ì½”ë“œ ì „ì²´ë¥¼ ì—¬ê¸°ì— ë¶™ì—¬ë„£ê¸°\n",
        "\n",
        "import csv\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "from typing import List, Dict, Union, Optional\n",
        "\n",
        "harmful_word_phrase_blacklist_data: Optional[List[Dict[str, str]]] = None\n",
        "\n",
        "def preprocess_text(text: str) -> str:\n",
        "    \"\"\"ì…ë ¥ í…ìŠ¤íŠ¸ì—ì„œ ë„ì–´ì“°ê¸° ë° ë¬¸ì¥ ë¶€í˜¸ë¥¼ ì œê±°í•©ë‹ˆë‹¤.\"\"\"\n",
        "    text_without_spaces = re.sub(r'\\s', '', text)\n",
        "    text_without_punctuation = re.sub(r'[.,?!\\'\"~Â·â€”â€¦ã€Œã€ã€Šã€‹ã€ˆã€‰ã€Šã€‹\\[\\]ã€ã€‘()\\[\\]]', '', text_without_spaces)\n",
        "    return text_without_punctuation\n",
        "\n",
        "def load_user_defined_blacklist_from_json(json_file_path: str) -> Union[List[Dict[str, str]], str, None]:\n",
        "    \"\"\"\n",
        "    JSON íŒŒì¼ì—ì„œ ì‚¬ìš©ì ì •ì˜ ìœ í•´ ë‹¨ì–´/êµ¬ë¬¸ ë¸”ë™ë¦¬ìŠ¤íŠ¸ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.\n",
        "    \"\"\"\n",
        "    user_defined_blacklist = []\n",
        "    if not os.path.exists(json_file_path):\n",
        "        print(f\"**[ê²½ê³ ]** ì‚¬ìš©ì ì •ì˜ ë¸”ë™ë¦¬ìŠ¤íŠ¸ íŒŒì¼ '{json_file_path}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ê¸°ë³¸ ë¸”ë™ë¦¬ìŠ¤íŠ¸ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        with open(json_file_path, 'r', encoding='utf-8') as json_file:\n",
        "            user_defined_data = json.load(json_file)\n",
        "            if not isinstance(user_defined_data, list):\n",
        "                error_message = f\"ì˜¤ë¥˜ ë°œìƒ: ì‚¬ìš©ì ì •ì˜ ë¸”ë™ë¦¬ìŠ¤íŠ¸ JSON íŒŒì¼ '{json_file_path}' í˜•ì‹ì´ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤. \\nJSON íŒŒì¼ì€ ë¦¬ìŠ¤íŠ¸ í˜•íƒœì—¬ì•¼ í•©ë‹ˆë‹¤.\"\n",
        "                return error_message\n",
        "            for item in user_defined_data:\n",
        "                if not isinstance(item, dict) or \"word_phrase\" not in item or \"category\" not in item:\n",
        "                    error_message = f\"ì˜¤ë¥˜ ë°œìƒ: ì‚¬ìš©ì ì •ì˜ ë¸”ë™ë¦¬ìŠ¤íŠ¸ JSON íŒŒì¼ '{json_file_path}' í˜•ì‹ì´ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤. \\nê° ì•„ì´í…œì€ 'word_phrase'ì™€ 'category' í‚¤ë¥¼ í¬í•¨í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ í˜•íƒœì—¬ì•¼ í•©ë‹ˆë‹¤.\"\n",
        "                    return error_message\n",
        "                user_defined_blacklist.append({\n",
        "                    \"word_phrase\": item[\"word_phrase\"].strip(),\n",
        "                    \"category\": item[\"category\"].strip()\n",
        "                })\n",
        "        return user_defined_blacklist\n",
        "\n",
        "    except json.JSONDecodeError as e:\n",
        "        error_message = f\"ì˜¤ë¥˜ ë°œìƒ: ì‚¬ìš©ì ì •ì˜ ë¸”ë™ë¦¬ìŠ¤íŠ¸ JSON íŒŒì¼ '{json_file_path}' ë‚´ìš© íŒŒì‹± ì˜¤ë¥˜ (JSON í˜•ì‹ì´ ì•„ë‹˜): {e}\"\n",
        "        return error_message\n",
        "    except Exception as e:\n",
        "        error_message = f\"ì˜¤ë¥˜ ë°œìƒ: ì‚¬ìš©ì ì •ì˜ ë¸”ë™ë¦¬ìŠ¤íŠ¸ JSON íŒŒì¼ '{json_file_path}' ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\"\n",
        "        return error_message\n",
        "\n",
        "def load_harmful_word_blacklist_from_csv(csv_file_path: str, user_blacklist_file_path: str = None) -> Union[str, None]:\n",
        "    \"\"\"\n",
        "    CSV íŒŒì¼ì—ì„œ ìœ í•´ ë‹¨ì–´/êµ¬ë¬¸ ë¸”ë™ë¦¬ìŠ¤íŠ¸ë¥¼ ë¡œë“œí•˜ê³ , ì‚¬ìš©ì ì •ì˜ ë¸”ë™ë¦¬ìŠ¤íŠ¸ì™€ ë³‘í•©í•©ë‹ˆë‹¤.\n",
        "    \"\"\"\n",
        "    global harmful_word_phrase_blacklist_data\n",
        "    harmful_word_phrase_blacklist: List[Dict[str, str]] = []\n",
        "    user_defined_blacklist: List[Dict[str, str]] = []\n",
        "\n",
        "    if not os.path.exists(csv_file_path):\n",
        "        error_message = f\"ì˜¤ë¥˜ ë°œìƒ: ë¸”ë™ë¦¬ìŠ¤íŠ¸ CSV íŒŒì¼ ê²½ë¡œ '{csv_file_path}'ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\"\n",
        "        return error_message\n",
        "\n",
        "    try:\n",
        "        with open(csv_file_path, 'r', encoding='utf-8') as csvfile:\n",
        "            try:\n",
        "                csv_reader = csv.DictReader(csvfile)\n",
        "                header = csv_reader.fieldnames\n",
        "                if not header or \"word_phrase\" not in header or \"category\" not in header:\n",
        "                    error_message = f\"ì˜¤ë¥˜ ë°œìƒ: ë¸”ë™ë¦¬ìŠ¤íŠ¸ CSV íŒŒì¼ '{csv_file_path}'ì˜ í˜•ì‹ì´ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤. \\n'word_phrase' ë° 'category' ì»¬ëŸ¼ì´ CSV í—¤ë”ì— í¬í•¨ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.\"\n",
        "                    return error_message\n",
        "                for row in csv_reader:\n",
        "                    harmful_word_phrase_blacklist.append({\n",
        "                        \"word_phrase\": row[\"word_phrase\"].strip(),\n",
        "                        \"category\": row[\"category\"].strip()\n",
        "                    })\n",
        "\n",
        "                if user_blacklist_file_path:\n",
        "                    user_defined_blacklist_result = load_user_defined_blacklist_from_json(user_blacklist_file_path)\n",
        "                    if isinstance(user_defined_blacklist_result, str):\n",
        "                        return user_defined_blacklist_result\n",
        "                    elif isinstance(user_defined_blacklist_result, list):\n",
        "                        user_defined_blacklist = user_defined_blacklist_result\n",
        "                        print(f\"**[ì„±ê³µ]** ì‚¬ìš©ì ì •ì˜ ë¸”ë™ë¦¬ìŠ¤íŠ¸ '{user_blacklist_file_path}' ë¡œë“œ ì™„ë£Œ. {len(user_defined_blacklist)}ê°œ ë‹¨ì–´/êµ¬ë¬¸ ì¶”ê°€.\")\n",
        "                    else:\n",
        "                        pass\n",
        "\n",
        "                merged_blacklist = user_defined_blacklist + harmful_word_phrase_blacklist\n",
        "                unique_blacklist_word_phrases = set()\n",
        "                unique_merged_blacklist: List[Dict[str, str]] = []\n",
        "                for item in merged_blacklist:\n",
        "                    if item[\"word_phrase\"] not in unique_blacklist_word_phrases:\n",
        "                        unique_merged_blacklist.append(item)\n",
        "                        unique_blacklist_word_phrases.add(item[\"word_phrase\"])\n",
        "                harmful_word_phrase_blacklist_data = unique_merged_blacklist\n",
        "                return None\n",
        "\n",
        "            except Exception as e:\n",
        "                error_message = f\"ì˜¤ë¥˜ ë°œìƒ: ë¸”ë™ë¦¬ìŠ¤íŠ¸ CSV íŒŒì¼ '{csv_file_path}'ì´ CSV í˜•ì‹ì´ ì•„ë‹ˆê±°ë‚˜, ë‚´ìš©ì„ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\"\n",
        "                return error_message\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"**[ë””ë²„ê¹… - ì˜ˆì™¸ ë°œìƒ]**: ì˜ˆì™¸ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤! - ì˜ˆì™¸ ì¢…ë¥˜: {type(e)}\")\n",
        "        error_message = f\"ì˜¤ë¥˜ ë°œìƒ: ë¸”ë™ë¦¬ìŠ¤íŠ¸ CSV íŒŒì¼ '{csv_file_path}' ë¡œë“œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. - {e}\"\n",
        "        print(error_message)\n",
        "        return error_message\n",
        "\n",
        "def detect_harmful_words_phrases(text: str, user_blacklist_file_path: str = None) -> Union[List[Dict[str, str]], str, None]:\n",
        "    \"\"\"í…ìŠ¤íŠ¸ì—ì„œ ìœ í•´ ë‹¨ì–´/êµ¬ë¬¸ì„ íƒì§€í•©ë‹ˆë‹¤.\"\"\"\n",
        "    global harmful_word_phrase_blacklist_data\n",
        "    if harmful_word_phrase_blacklist_data is None: # ë¸”ë™ë¦¬ìŠ¤íŠ¸ ë°ì´í„°ê°€ None ì¸ ê²½ìš°, ë¡œë“œ ì‹œë„\n",
        "        blacklist_file_path = \"harmful_words_phrases_blacklist.csv\"\n",
        "        error_message = load_harmful_word_blacklist_from_csv(blacklist_file_path, user_blacklist_file_path) # ì‚¬ìš©ì ì •ì˜ ë¸”ë™ë¦¬ìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œ ì „ë‹¬\n",
        "        if error_message: # ë¸”ë™ë¦¬ìŠ¤íŠ¸ ë¡œë“œ ì‹¤íŒ¨ ì‹œ ì˜¤ë¥˜ ë©”ì‹œì§€ ë°˜í™˜\n",
        "            return error_message\n",
        "        if harmful_word_phrase_blacklist_data is None: # ì—¬ì „íˆ None ì¸ ê²½ìš° (ë¡œë“œ ì‹¤íŒ¨)\n",
        "            return None # None ë°˜í™˜ (ì˜¤ë¥˜)\n",
        "\n",
        "    preprocessed_text = preprocess_text(text) # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ (ë„ì–´ì“°ê¸°, ë¬¸ì¥ ë¶€í˜¸ ì œê±°)\n",
        "    detected_words_info: List[Dict[str, str]] = [] # íƒì§€ëœ ë‹¨ì–´/êµ¬ë¬¸ ì •ë³´ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”\n",
        "    for item in harmful_word_phrase_blacklist_data: # ë¸”ë™ë¦¬ìŠ¤íŠ¸ ìˆœíšŒ\n",
        "        word_phrase = item[\"word_phrase\"] # ë¸”ë™ë¦¬ìŠ¤íŠ¸ ë‹¨ì–´/êµ¬ë¬¸\n",
        "        category = item[\"category\"] # ë¸”ë™ë¦¬ìŠ¤íŠ¸ ì¹´í…Œê³ ë¦¬\n",
        "        if word_phrase in preprocessed_text: # ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ì— ë¸”ë™ë¦¬ìŠ¤íŠ¸ ë‹¨ì–´/êµ¬ë¬¸ í¬í•¨ ì—¬ë¶€ í™•ì¸\n",
        "            detected_words_info.append({\"word_phrase\": word_phrase, \"category\": category}) # íƒì§€ëœ ë‹¨ì–´/êµ¬ë¬¸ ì •ë³´ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€\n",
        "    return detected_words_info # íƒì§€ëœ ë‹¨ì–´/êµ¬ë¬¸ ì •ë³´ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜\n",
        "\n",
        "def add_word_to_blacklist(word_phrase: str, category: str) -> str:\n",
        "    \"\"\"ë‹¨ì–´/êµ¬ë¬¸ì„ ë¸”ë™ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€í•©ë‹ˆë‹¤.\"\"\"\n",
        "    global harmful_word_phrase_blacklist_data\n",
        "    if not word_phrase:\n",
        "        return \"ì˜¤ë¥˜: ë‹¨ì–´/êµ¬ë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.\"\n",
        "    new_item = {\"word_phrase\": word_phrase, \"category\": category}\n",
        "    if harmful_word_phrase_blacklist_data is None:\n",
        "        harmful_word_phrase_blacklist_data = [new_item]\n",
        "    else:\n",
        "        harmful_word_phrase_blacklist_data.append(new_item)\n",
        "    update_blacklist_in_csv() # CSV íŒŒì¼ ì—…ë°ì´íŠ¸\n",
        "    return f\"'{word_phrase}' ({category}) ì¹´í…Œê³ ë¦¬ë¡œ ë¸”ë™ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€ ì™„ë£Œ\"\n",
        "\n",
        "def delete_word_from_blacklist(word_phrase: str) -> str:\n",
        "    \"\"\"ë‹¨ì–´/êµ¬ë¬¸ì„ ë¸”ë™ë¦¬ìŠ¤íŠ¸ì—ì„œ ì‚­ì œí•©ë‹ˆë‹¤.\"\"\"\n",
        "    global harmful_word_phrase_blacklist_data\n",
        "    if not word_phrase:\n",
        "        return \"ì˜¤ë¥˜: ì‚­ì œí•  ë‹¨ì–´/êµ¬ë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.\"\n",
        "    if harmful_word_phrase_blacklist_data:\n",
        "        harmful_word_phrase_blacklist_data = [item for item in harmful_word_phrase_blacklist_data if item[\"word_phrase\"] != word_phrase]\n",
        "        update_blacklist_in_csv() # CSV íŒŒì¼ ì—…ë°ì´íŠ¸\n",
        "        return f\"'{word_phrase}' ë¸”ë™ë¦¬ìŠ¤íŠ¸ì—ì„œ ì‚­ì œ ì™„ë£Œ\"\n",
        "    else:\n",
        "        return \"ì˜¤ë¥˜: ë¸”ë™ë¦¬ìŠ¤íŠ¸ì— ì‚­ì œí•  ë‹¨ì–´ê°€ ì—†ìŠµë‹ˆë‹¤.\"\n",
        "\n",
        "def update_blacklist_in_csv():\n",
        "    \"\"\"ë¸”ë™ë¦¬ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ CSV íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤.\"\"\"\n",
        "    global harmful_word_phrase_blacklist_data\n",
        "    if harmful_word_phrase_blacklist_data is None:\n",
        "        return\n",
        "    blacklist_file_path = \"/content/drive/MyDrive/AI_Vtuber/harmful_words_phrases_blacklist.csv\" # Google Drive ê²½ë¡œ ëª…ì‹œ\n",
        "    try:\n",
        "        with open(blacklist_file_path, 'w', encoding='utf-8', newline='') as csvfile:\n",
        "            fieldnames = ['word_phrase', 'category']\n",
        "            csv_writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "            csv_writer.writeheader()\n",
        "            csv_writer.writerows(harmful_word_phrase_blacklist_data)\n",
        "        print(f\"**[ì„±ê³µ]** ë¸”ë™ë¦¬ìŠ¤íŠ¸ CSV íŒŒì¼ '{blacklist_file_path}' ì—…ë°ì´íŠ¸ ì™„ë£Œ\")\n",
        "    except Exception as e:\n",
        "        error_message = f\"**[ì˜¤ë¥˜]** ë¸”ë™ë¦¬ìŠ¤íŠ¸ CSV íŒŒì¼ '{blacklist_file_path}' ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}\"\n",
        "        print(error_message)\n",
        "\n",
        "def load_blacklist_data(): # ë¸”ë™ë¦¬ìŠ¤íŠ¸ ë¡œë“œ í•¨ìˆ˜ (UI ê´€ë ¨ ì½”ë“œ ì œê±°)\n",
        "    global harmful_word_phrase_blacklist_data\n",
        "    blacklist_file_path = \"/content/drive/MyDrive/AI_Vtuber/harmful_words_phrases_blacklist.csv\" # Google Drive ê²½ë¡œ ëª…ì‹œ\n",
        "    user_blacklist_file_path = \"user_defined_blacklist.json\"\n",
        "    error_message = load_harmful_word_blacklist_from_csv(blacklist_file_path, user_blacklist_file_path)\n",
        "    if error_message:\n",
        "        print(f\"ë¸”ë™ë¦¬ìŠ¤íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {error_message}\")\n",
        "        harmful_word_phrase_blacklist_data = []\n",
        "    elif harmful_word_phrase_blacklist_data is None:\n",
        "        print(\"ë¸”ë™ë¦¬ìŠ¤íŠ¸ ë¡œë“œ ì‹¤íŒ¨ (ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜)\")\n",
        "        harmful_word_phrase_blacklist_data = []\n",
        "    else:\n",
        "        print(\"ë¸”ë™ë¦¬ìŠ¤íŠ¸ ë¡œë“œ ì™„ë£Œ\")\n",
        "\n",
        "# --- [UI ê´€ë ¨ ì½”ë“œ ì œê±°] ---\n",
        "# UI ìš”ì†Œ ì •ì˜, ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬, UI í‘œì‹œ ì½”ë“œ ëª¨ë‘ ì œê±°ë¨\n",
        "\n",
        "# --- [í…ŒìŠ¤íŠ¸ ì½”ë“œ] ---\n",
        "if __name__ == \"__main__\": # __name__ == \"__main__\" ë¸”ë¡ ì¶”ê°€ (ë…ë¦½ ì‹¤í–‰ ì‹œì—ë§Œ í…ŒìŠ¤íŠ¸ ì½”ë“œ ì‹¤í–‰)\n",
        "    load_blacklist_data() # ë¸”ë™ë¦¬ìŠ¤íŠ¸ ë¡œë“œ\n",
        "\n",
        "    # í…ìŠ¤íŠ¸ ì…ë ¥ ë° ìœ í•´ ë‹¨ì–´/êµ¬ë¬¸ íƒì§€ í…ŒìŠ¤íŠ¸\n",
        "    test_text = \"ì´ëŸ° ë©ì²­ì´ ê°™ì€\" # í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸\n",
        "    detected_words = detect_harmful_words_phrases(test_text) # ìœ í•´ ë‹¨ì–´/êµ¬ë¬¸ íƒì§€ í•¨ìˆ˜ í˜¸ì¶œ\n",
        "    if detected_words: # íƒì§€ëœ ìœ í•´ ë‹¨ì–´/êµ¬ë¬¸ì´ ìˆëŠ” ê²½ìš°\n",
        "        print(f\"\\n**[íƒì§€ ê²°ê³¼]** í…ìŠ¤íŠ¸: '{test_text}' ì—ì„œ ìœ í•´ ë‹¨ì–´/êµ¬ë¬¸ì´ íƒì§€ë˜ì—ˆìŠµë‹ˆë‹¤:\") # ê²°ê³¼ ë©”ì‹œì§€ ì¶œë ¥\n",
        "        for word in detected_words: # íƒì§€ëœ ë‹¨ì–´/êµ¬ë¬¸ ëª©ë¡ ì¶œë ¥\n",
        "            print(f\"- '{word['word_phrase']}' (ì¹´í…Œê³ ë¦¬: {word['category']})\")\n",
        "    else: # íƒì§€ëœ ìœ í•´ ë‹¨ì–´/êµ¬ë¬¸ì´ ì—†ëŠ” ê²½ìš°\n",
        "        print(f\"\\n**[íƒì§€ ê²°ê³¼]** í…ìŠ¤íŠ¸: '{test_text}' ì—ì„œ ìœ í•´ ë‹¨ì–´/êµ¬ë¬¸ì´ íƒì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\") # ê²°ê³¼ ë©”ì‹œì§€ ì¶œë ¥\n",
        "\n",
        "    # ë¸”ë™ë¦¬ìŠ¤íŠ¸ ë‹¨ì–´/êµ¬ë¬¸ ì¶”ê°€ í…ŒìŠ¤íŠ¸\n",
        "    word_to_add = \"test_word\" # ì¶”ê°€í•  ë‹¨ì–´/êµ¬ë¬¸\n",
        "    category_to_add = \"ì¼ë°˜\" # ì¶”ê°€í•  ì¹´í…Œê³ ë¦¬\n",
        "    add_result_message = add_word_to_blacklist(word_to_add, category_to_add) # ë¸”ë™ë¦¬ìŠ¤íŠ¸ ì¶”ê°€ í•¨ìˆ˜ í˜¸ì¶œ\n",
        "    print(f\"\\n**[ë¸”ë™ë¦¬ìŠ¤íŠ¸ ì¶”ê°€ ê²°ê³¼]**: {add_result_message}\") # ì¶”ê°€ ê²°ê³¼ ë©”ì‹œì§€ ì¶œë ¥\n",
        "\n",
        "    # ë¸”ë™ë¦¬ìŠ¤íŠ¸ ë‹¨ì–´/êµ¬ë¬¸ ì‚­ì œ í…ŒìŠ¤íŠ¸\n",
        "    word_to_delete = \"test_word\" # ì‚­ì œí•  ë‹¨ì–´/êµ¬ë¬¸ (ë°©ê¸ˆ ì¶”ê°€í•œ ë‹¨ì–´)\n",
        "    delete_result_message = delete_word_from_blacklist(word_to_delete) # ë¸”ë™ë¦¬ìŠ¤íŠ¸ ì‚­ì œ í•¨ìˆ˜ í˜¸ì¶œ\n",
        "    print(f\"\\n**[ë¸”ë™ë¦¬ìŠ¤íŠ¸ ì‚­ì œ ê²°ê³¼]**: {delete_result_message}\") # ì‚­ì œ ê²°ê³¼ ë©”ì‹œì§€ ì¶œë ¥\n",
        "\n",
        "    # ë¸”ë™ë¦¬ìŠ¤íŠ¸ ë°ì´í„° (harmful_word_phrase_blacklist_data) ë‚´ìš© í™•ì¸ (ë””ë²„ê¹… ìš©ë„)\n",
        "    print(f\"\\n**[ìµœì¢… ë¸”ë™ë¦¬ìŠ¤íŠ¸ ë°ì´í„°]**:\") # ìµœì¢… ë¸”ë™ë¦¬ìŠ¤íŠ¸ ë°ì´í„° ì œëª© ì¶œë ¥\n",
        "    if harmful_word_phrase_blacklist_data: # ë¸”ë™ë¦¬ìŠ¤íŠ¸ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°\n",
        "        for item in harmful_word_phrase_blacklist_data: # ë¸”ë™ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œ ìˆœíšŒí•˜ë©° ì¶œë ¥\n",
        "            print(f\"- '{item['word_phrase']}' (ì¹´í…Œê³ ë¦¬: {item['category']})\") # ê° ì•„ì´í…œ ì •ë³´ ì¶œë ¥\n",
        "    else: # ë¸”ë™ë¦¬ìŠ¤íŠ¸ ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš°\n",
        "        print(\"ë¸”ë™ë¦¬ìŠ¤íŠ¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\") # ë¸”ë™ë¦¬ìŠ¤íŠ¸ ì—†ìŒ ë©”ì‹œì§€ ì¶œë ¥"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tq3CEGdKdq3D",
        "outputId": "9e3feb87-4fa8-4c0d-cef8-598451c65fd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**[ê²½ê³ ]** ì‚¬ìš©ì ì •ì˜ ë¸”ë™ë¦¬ìŠ¤íŠ¸ íŒŒì¼ 'user_defined_blacklist.json'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ê¸°ë³¸ ë¸”ë™ë¦¬ìŠ¤íŠ¸ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
            "ë¸”ë™ë¦¬ìŠ¤íŠ¸ ë¡œë“œ ì™„ë£Œ\n",
            "\n",
            "**[íƒì§€ ê²°ê³¼]** í…ìŠ¤íŠ¸: 'ì´ëŸ° ë©ì²­ì´ ê°™ì€' ì—ì„œ ìœ í•´ ë‹¨ì–´/êµ¬ë¬¸ì´ íƒì§€ë˜ì—ˆìŠµë‹ˆë‹¤:\n",
            "- 'ë©ì²­ì´' (ì¹´í…Œê³ ë¦¬: ìš•ì„¤)\n",
            "**[ì„±ê³µ]** ë¸”ë™ë¦¬ìŠ¤íŠ¸ CSV íŒŒì¼ '/content/drive/MyDrive/AI_Vtuber/harmful_words_phrases_blacklist.csv' ì—…ë°ì´íŠ¸ ì™„ë£Œ\n",
            "\n",
            "**[ë¸”ë™ë¦¬ìŠ¤íŠ¸ ì¶”ê°€ ê²°ê³¼]**: 'test_word' (ì¼ë°˜) ì¹´í…Œê³ ë¦¬ë¡œ ë¸”ë™ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€ ì™„ë£Œ\n",
            "**[ì„±ê³µ]** ë¸”ë™ë¦¬ìŠ¤íŠ¸ CSV íŒŒì¼ '/content/drive/MyDrive/AI_Vtuber/harmful_words_phrases_blacklist.csv' ì—…ë°ì´íŠ¸ ì™„ë£Œ\n",
            "\n",
            "**[ë¸”ë™ë¦¬ìŠ¤íŠ¸ ì‚­ì œ ê²°ê³¼]**: 'test_word' ë¸”ë™ë¦¬ìŠ¤íŠ¸ì—ì„œ ì‚­ì œ ì™„ë£Œ\n",
            "\n",
            "**[ìµœì¢… ë¸”ë™ë¦¬ìŠ¤íŠ¸ ë°ì´í„°]**:\n",
            "- 'ë©ì²­ì´' (ì¹´í…Œê³ ë¦¬: ìš•ì„¤)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# í•„ìš”í•œ íŒŒì¼ ê²½ë¡œ ì„¤ì •\n",
        "blacklist_file_path = \"/content/drive/MyDrive/AI_Vtuber/harmful_words_phrases_blacklist.csv\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3_NczkxdusE",
        "outputId": "6efc3419-35b6-4205-d514-44609e6dcc9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# AI API ì—°ê²° ì½”ë“œ (ì €ë¥¼ í†µí•´ ì œê³µë°›ì€ ì½”ë“œ ë¶™ì—¬ë„£ê¸°)"
      ],
      "metadata": {
        "id": "RAxuzmWRd0IT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ ì‹¤í–‰"
      ],
      "metadata": {
        "id": "Hr772S9rd1wS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Google Drive ì—°ê²° (ì´ë¯¸ ì—°ê²°ë˜ì–´ ìˆë‹¤ë©´ ì´ ë¶€ë¶„ì€ ê±´ë„ˆë›°ì–´ë„ ë©ë‹ˆë‹¤.)\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "def add_word_to_blacklist_csv(file_path: str, word_phrase: str, category: str):\n",
        "    \"\"\"ì£¼ì–´ì§„ CSV íŒŒì¼ì— ë‹¨ì–´/êµ¬ë¬¸ì„ ë¸”ë™ë¦¬ìŠ¤íŠ¸ë¡œ ì¶”ê°€í•©ë‹ˆë‹¤.\"\"\"\n",
        "    try:\n",
        "        # íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ í—¤ë”ë¥¼ í¬í•¨í•˜ì—¬ ìƒˆë¡œ ìƒì„±\n",
        "        if not os.path.exists(file_path):\n",
        "            with open(file_path, 'w', encoding='utf-8', newline='') as csvfile:\n",
        "                writer = csv.writer(csvfile)\n",
        "                writer.writerow(['word_phrase', 'category'])\n",
        "                writer.writerow([word_phrase, category])\n",
        "            print(f\"**[ì„±ê³µ]** ë¸”ë™ë¦¬ìŠ¤íŠ¸ íŒŒì¼ '{file_path}' ìƒì„± ë° '{word_phrase}' ì¶”ê°€ ì™„ë£Œ\")\n",
        "        else:\n",
        "            # íŒŒì¼ì´ ì¡´ì¬í•˜ë©´ ì¶”ê°€ ëª¨ë“œë¡œ ì—´ì–´ ë‹¨ì–´ ì¶”ê°€\n",
        "            with open(file_path, 'a', encoding='utf-8', newline='') as csvfile:\n",
        "                writer = csv.writer(csvfile)\n",
        "                writer.writerow([word_phrase, category])\n",
        "            print(f\"**[ì„±ê³µ]** ë¸”ë™ë¦¬ìŠ¤íŠ¸ íŒŒì¼ '{file_path}'ì— '{word_phrase}' ì¶”ê°€ ì™„ë£Œ\")\n",
        "    except Exception as e:\n",
        "        error_message = f\"**[ì˜¤ë¥˜]** ë¸”ë™ë¦¬ìŠ¤íŠ¸ íŒŒì¼ '{file_path}'ì— '{word_phrase}' ì¶”ê°€ ì‹¤íŒ¨: {e}\"\n",
        "        print(error_message)\n",
        "\n",
        "# ì¶”ê°€í•˜ê³  ì‹¶ì€ ë‹¨ì–´ì™€ ì¹´í…Œê³ ë¦¬ ì„¤ì •\n",
        "word_to_add = \"ë©ì²­ì´\"\n",
        "category_to_add = \"ìš•ì„¤\"\n",
        "\n",
        "# ë¸”ë™ë¦¬ìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œ (ë³¸ì¸ì˜ Google Drive ê²½ë¡œì— ë§ì¶° ìˆ˜ì •í•´ì£¼ì„¸ìš”)\n",
        "blacklist_file_path = \"/content/drive/MyDrive/AI_Vtuber/harmful_words_phrases_blacklist.csv\"\n",
        "\n",
        "# í•¨ìˆ˜ í˜¸ì¶œí•˜ì—¬ ë‹¨ì–´ ì¶”ê°€\n",
        "add_word_to_blacklist_csv(blacklist_file_path, word_to_add, category_to_add)\n",
        "\n",
        "# (ì„ íƒ ì‚¬í•­) ë‹¤ë¥¸ ë‹¨ì–´ ì¶”ê°€ ì˜ˆì‹œ\n",
        "# add_word_to_blacklist_csv(blacklist_file_path, \"ë°”ë³´\", \"ë¹„ì†ì–´\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N11kFSbqif3z",
        "outputId": "cdca4be1-3636-492b-e18d-8a21e919ab6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "**[ì„±ê³µ]** ë¸”ë™ë¦¬ìŠ¤íŠ¸ íŒŒì¼ '/content/drive/MyDrive/AI_Vtuber/harmful_words_phrases_blacklist.csv' ìƒì„± ë° 'ë©ì²­ì´' ì¶”ê°€ ì™„ë£Œ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python ai_interface.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9fWVZV_ZbZG",
        "outputId": "e86cda1b-4bdd-4548-e9d0-ccc76d99d5e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì‚¬ìš©ì: ì•ˆë…•í•˜ì„¸ìš”?\n",
            "AI ì‘ë‹µ: ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”? ğŸ˜Š\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from ai_interface import send_message_to_ai\n",
        "from main import detect_harmful_words_phrases # ìœ í•´ ë‹¨ì–´ íƒì§€ í•¨ìˆ˜ ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "\n",
        "def start_chat():\n",
        "    print(\"AI ì±„íŒ…ì„ ì‹œì‘í•©ë‹ˆë‹¤. 'ì¢…ë£Œ'ë¥¼ ì…ë ¥í•˜ë©´ ì±„íŒ…ì´ ì¢…ë£Œë©ë‹ˆë‹¤.\")\n",
        "    while True:\n",
        "        user_input = input(\"ì‚¬ìš©ì: \")\n",
        "        if user_input.lower() == 'ì¢…ë£Œ':\n",
        "            print(\"ì±„íŒ…ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.\")\n",
        "            break\n",
        "\n",
        "        # ì‚¬ìš©ì ì…ë ¥ì— ëŒ€í•œ ìœ í•´ ë‹¨ì–´ í•„í„°ë§\n",
        "        detected_harmful_words_user = detect_harmful_words_phrases(user_input)\n",
        "        if detected_harmful_words_user:\n",
        "            print(f\"**[ê²½ê³ ]** ì…ë ¥ì— ìœ í•´ ë‹¨ì–´/êµ¬ë¬¸ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤: {detected_harmful_words_user}\")\n",
        "            continue # ìœ í•´ ë‹¨ì–´ê°€ ìˆìœ¼ë©´ AIì—ê²Œ ë³´ë‚´ì§€ ì•Šê³  ë‹¤ì‹œ ì…ë ¥ ë°›ìŒ\n",
        "\n",
        "        ai_response = send_message_to_ai(user_input)\n",
        "\n",
        "        # AI ì‘ë‹µì— ëŒ€í•œ ìœ í•´ ë‹¨ì–´ í•„í„°ë§\n",
        "        detected_harmful_words_ai = detect_harmful_words_phrases(ai_response)\n",
        "        if detected_harmful_words_ai:\n",
        "            print(f\"**[ê²½ê³ ]** AI ì‘ë‹µì— ìœ í•´ ë‹¨ì–´/êµ¬ë¬¸ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤: {detected_harmful_words_ai}\")\n",
        "            print(\"**[ì£¼ì˜]** AI ì‘ë‹µì—ì„œ ìœ í•´ ë‹¨ì–´ê°€ ê°ì§€ë˜ì–´ í‘œì‹œí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\")\n",
        "            ai_response = \"[ìœ í•´ ë‹¨ì–´ ê°ì§€ë¨]\" # ë˜ëŠ” ë‹¤ë¥¸ ëŒ€ì²´ ë©”ì‹œì§€ ì²˜ë¦¬\n",
        "        else:\n",
        "            print(f\"AI ì‘ë‹µ: {ai_response}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    start_chat()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "-I7zddMpgEld",
        "outputId": "f4059c7f-0cd6-407c-971a-fa955b96a446"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (main.py, line 4)",
          "traceback": [
            "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
            "  File \u001b[1;32m\"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3553\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-16-32a7c4385180>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0;36m, in \u001b[0;35m<cell line: 0>\u001b[0;36m\u001b[0m\n\u001b[0;31m    from main import detect_harmful_words_phrases # ìœ í•´ ë‹¨ì–´ íƒì§€ í•¨ìˆ˜ ë¶ˆëŸ¬ì˜¤ê¸°\u001b[0m\n",
            "\u001b[0;36m  File \u001b[0;32m\"/content/main.py\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    detected_harmful =\u001b[0m\n\u001b[0m                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2cprV3tlZQc",
        "outputId": "469a305f-9b2f-4d0d-ab39-a80f69530975"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  File \"/content/main.py\", line 4\n",
            "    detected_harmful =\n",
            "                      ^\n",
            "SyntaxError: invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "from typing import List, Dict, Optional\n",
        "from google.colab import drive\n",
        "\n",
        "# --- Google Drive ë§ˆìš´íŠ¸ ë° íŒŒì¼ ì²˜ë¦¬ ---\n",
        "\n",
        "def mount_google_drive():\n",
        "    \"\"\"êµ¬ê¸€ ë“œë¼ì´ë¸Œë¥¼ Colab í™˜ê²½ì— ë§ˆìš´íŠ¸\"\"\"\n",
        "    drive.mount('/content/drive')  # /content/drive ê²½ë¡œë¡œ êµ¬ê¸€ ë“œë¼ì´ë¸Œê°€ ë§ˆìš´íŠ¸ë¨\n",
        "\n",
        "# --- ë¸”ë™ë¦¬ìŠ¤íŠ¸ ë¡œë”© ë° ê´€ë¦¬ í•¨ìˆ˜ ---\n",
        "\n",
        "class BlacklistLoader:\n",
        "    def __init__(self, csv_path: str, json_path: Optional[str] = None):\n",
        "        \"\"\"CSV íŒŒì¼ ê²½ë¡œ, JSON íŒŒì¼ ê²½ë¡œë¥¼ ë°›ì•„ ë¸”ë™ë¦¬ìŠ¤íŠ¸ë¥¼ ì´ˆê¸°í™”\"\"\"\n",
        "        self.csv_path = csv_path\n",
        "        self.json_path = json_path\n",
        "        self.blacklist_data: List[Dict[str, str]] = []\n",
        "        self.load_blacklist()\n",
        "\n",
        "    def load_blacklist(self):\n",
        "        \"\"\"ë¸”ë™ë¦¬ìŠ¤íŠ¸ ë¡œë”©: CSV ë° JSON íŒŒì¼ì—ì„œ ë¶ˆëŸ¬ì˜¤ê¸°\"\"\"\n",
        "        csv_data = self._load_csv()\n",
        "        json_data = self._load_json() if self.json_path else []\n",
        "        # CSVì™€ JSON ë°ì´í„°ë¥¼ ë³‘í•©í•˜ì—¬ ìµœì¢… ë¸”ë™ë¦¬ìŠ¤íŠ¸ ë°ì´í„° ìƒì„±\n",
        "        self.blacklist_data = self._merge_blacklists(csv_data, json_data)\n",
        "\n",
        "    def _load_csv(self) -> List[Dict[str, str]]:\n",
        "        \"\"\"CSV íŒŒì¼ì—ì„œ ë¸”ë™ë¦¬ìŠ¤íŠ¸ ë¡œë“œ\"\"\"\n",
        "        if not os.path.exists(self.csv_path):\n",
        "            print(f\"[ê²½ê³ ] ë¸”ë™ë¦¬ìŠ¤íŠ¸ CSV íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {self.csv_path}\")\n",
        "            return []\n",
        "\n",
        "        try:\n",
        "            with open(self.csv_path, 'r', encoding='utf-8') as csvfile:\n",
        "                csv_reader = csv.DictReader(csvfile)\n",
        "                if \"word_phrase\" not in csv_reader.fieldnames or \"category\" not in csv_reader.fieldnames:\n",
        "                    print(f\"[ì˜¤ë¥˜] CSV í˜•ì‹ì´ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤: {self.csv_path}\")\n",
        "                    return []\n",
        "                return [\n",
        "                    {\"word_phrase\": row[\"word_phrase\"].strip(), \"category\": row[\"category\"].strip()}\n",
        "                    for row in csv_reader\n",
        "                ]\n",
        "        except Exception as e:\n",
        "            print(f\"[ì˜¤ë¥˜] CSV ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
        "            return []\n",
        "\n",
        "    def _load_json(self) -> List[Dict[str, str]]:\n",
        "        \"\"\"JSON íŒŒì¼ì—ì„œ ì‚¬ìš©ì ì •ì˜ ë¸”ë™ë¦¬ìŠ¤íŠ¸ ë¡œë“œ\"\"\"\n",
        "        if not os.path.exists(self.json_path):\n",
        "            print(f\"[ê²½ê³ ] ì‚¬ìš©ì ì •ì˜ ë¸”ë™ë¦¬ìŠ¤íŠ¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {self.json_path}\")\n",
        "            return []\n",
        "\n",
        "        try:\n",
        "            with open(self.json_path, 'r', encoding='utf-8') as json_file:\n",
        "                data = json.load(json_file)\n",
        "                if not isinstance(data, list):\n",
        "                    print(f\"[ì˜¤ë¥˜] JSON í˜•ì‹ì´ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤: {self.json_path}\")\n",
        "                    return []\n",
        "                return [\n",
        "                    {\"word_phrase\": item[\"word_phrase\"].strip(), \"category\": item[\"category\"].strip()}\n",
        "                    for item in data if isinstance(item, dict) and \"word_phrase\" in item and \"category\" in item\n",
        "                ]\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"[ì˜¤ë¥˜] JSON íŒŒì‹± ì˜¤ë¥˜: {e}\")\n",
        "            return []\n",
        "        except Exception as e:\n",
        "            print(f\"[ì˜¤ë¥˜] JSON ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
        "            return []\n",
        "\n",
        "    def _merge_blacklists(self, csv_data: List[Dict[str, str]], json_data: List[Dict[str, str]]) -> List[Dict[str, str]]:\n",
        "        \"\"\"CSV ë° JSON ë°ì´í„°ë¥¼ ë³‘í•©í•˜ê³  ì¤‘ë³µ ì œê±°\"\"\"\n",
        "        unique_entries = {item[\"word_phrase\"]: item for item in csv_data + json_data}\n",
        "        return list(unique_entries.values())\n",
        "\n",
        "    def save_blacklist(self):\n",
        "        \"\"\"ë¸”ë™ë¦¬ìŠ¤íŠ¸ë¥¼ CSV íŒŒì¼ì— ì €ì¥\"\"\"\n",
        "        try:\n",
        "            with open(self.csv_path, 'w', encoding='utf-8', newline='') as csvfile:\n",
        "                fieldnames = ['word_phrase', 'category']\n",
        "                csv_writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "                csv_writer.writeheader()\n",
        "                csv_writer.writerows(self.blacklist_data)\n",
        "            print(f\"[ì„±ê³µ] ë¸”ë™ë¦¬ìŠ¤íŠ¸ ì €ì¥ ì™„ë£Œ: {self.csv_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"[ì˜¤ë¥˜] ë¸”ë™ë¦¬ìŠ¤íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}\")\n",
        "\n",
        "    def add_to_blacklist(self, word_phrase: str, category: str):\n",
        "        \"\"\"ë¸”ë™ë¦¬ìŠ¤íŠ¸ì— í•­ëª© ì¶”ê°€\"\"\"\n",
        "        if any(item[\"word_phrase\"] == word_phrase for item in self.blacklist_data):\n",
        "            print(f\"[ì •ë³´] '{word_phrase}' ì´ë¯¸ ë¸”ë™ë¦¬ìŠ¤íŠ¸ì— ì¡´ì¬í•¨\")\n",
        "            return\n",
        "        self.blacklist_data.append({\"word_phrase\": word_phrase, \"category\": category})\n",
        "        self.save_blacklist()\n",
        "\n",
        "    def remove_from_blacklist(self, word_phrase: str):\n",
        "        \"\"\"ë¸”ë™ë¦¬ìŠ¤íŠ¸ì—ì„œ í•­ëª© ì œê±°\"\"\"\n",
        "        self.blacklist_data = [item for item in self.blacklist_data if item[\"word_phrase\"] != word_phrase]\n",
        "        self.save_blacklist()\n",
        "\n",
        "# --- í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë° ìœ í•´ ë‹¨ì–´ íƒì§€ í•¨ìˆ˜ ---\n",
        "\n",
        "def preprocess_text(text: str) -> str:\n",
        "    \"\"\"ì…ë ¥ í…ìŠ¤íŠ¸ì—ì„œ ë„ì–´ì“°ê¸° ë° ë¬¸ì¥ ë¶€í˜¸ë¥¼ ì œê±°\"\"\"\n",
        "    text_without_spaces = re.sub(r'\\s', '', text)\n",
        "    return re.sub(r'[.,?!\\'\"~Â·â€”â€¦ã€Œã€ã€Šã€‹ã€ˆã€‰ã€Šã€‹\\[\\]ã€ã€‘()\\[\\]]', '', text_without_spaces)\n",
        "\n",
        "def detect_harmful_words(text: str, blacklist_loader: BlacklistLoader) -> List[Dict[str, str]]:\n",
        "    \"\"\"í…ìŠ¤íŠ¸ì—ì„œ ìœ í•´ ë‹¨ì–´/êµ¬ë¬¸ì„ íƒì§€\"\"\"\n",
        "    preprocessed_text = preprocess_text(text)\n",
        "    return [\n",
        "        {\"word_phrase\": item[\"word_phrase\"], \"category\": item[\"category\"]}\n",
        "        for item in blacklist_loader.blacklist_data if item[\"word_phrase\"] in preprocessed_text\n",
        "    ]\n",
        "\n",
        "# --- í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ---\n",
        "if __name__ == \"__main__\":\n",
        "    # êµ¬ê¸€ ë“œë¼ì´ë¸Œ ë§ˆìš´íŠ¸\n",
        "    mount_google_drive()\n",
        "\n",
        "    # ë¸”ë™ë¦¬ìŠ¤íŠ¸ ë¡œë”© (ê²½ë¡œ ìˆ˜ì •)\n",
        "    blacklist_loader = BlacklistLoader(\n",
        "        csv_path=\"/content/drive/MyDrive/AI_Vtuber/harmful_words_phrases_blacklist.csv\",\n",
        "        json_path=\"/content/drive/MyDrive/AI_Vtuber/user_defined_blacklist.json\"\n",
        "    )\n",
        "\n",
        "    # í…ìŠ¤íŠ¸ì—ì„œ ìœ í•´ ë‹¨ì–´ íƒì§€\n",
        "    test_text = \"ì´ëŸ° ë©ì²­ì´ ê°™ì€\"\n",
        "    detected_words = detect_harmful_words(test_text, blacklist_loader)\n",
        "\n",
        "    print(\"\\n[íƒì§€ ê²°ê³¼]\")\n",
        "    if detected_words:\n",
        "        for word in detected_words:\n",
        "            print(f\"- '{word['word_phrase']}' (ì¹´í…Œê³ ë¦¬: {word['category']})\")\n",
        "    else:\n",
        "        print(\"ìœ í•´ ë‹¨ì–´/êµ¬ë¬¸ì´ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "    # ë¸”ë™ë¦¬ìŠ¤íŠ¸ ê´€ë¦¬ ì˜ˆì‹œ\n",
        "    blacklist_loader.add_to_blacklist(\"ìƒˆë¡œìš´ê¸ˆì§€ì–´\", \"ì¼ë°˜\")\n",
        "    blacklist_loader.remove_from_blacklist(\"ìƒˆë¡œìš´ê¸ˆì§€ì–´\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VmXqN6SryCY",
        "outputId": "5b8aa37f-5c9e-421e-8245-8c4dbcad7b5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[ê²½ê³ ] ì‚¬ìš©ì ì •ì˜ ë¸”ë™ë¦¬ìŠ¤íŠ¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: /content/drive/MyDrive/AI_Vtuber/user_defined_blacklist.json\n",
            "\n",
            "[íƒì§€ ê²°ê³¼]\n",
            "- 'ë©ì²­ì´' (ì¹´í…Œê³ ë¦¬: ìš•ì„¤)\n",
            "[ì„±ê³µ] ë¸”ë™ë¦¬ìŠ¤íŠ¸ ì €ì¥ ì™„ë£Œ: /content/drive/MyDrive/AI_Vtuber/harmful_words_phrases_blacklist.csv\n",
            "[ì„±ê³µ] ë¸”ë™ë¦¬ìŠ¤íŠ¸ ì €ì¥ ì™„ë£Œ: /content/drive/MyDrive/AI_Vtuber/harmful_words_phrases_blacklist.csv\n"
          ]
        }
      ]
    }
  ]
}